\newpage
\section{Autonomous Driving Vehicles}
The \textbf{Automatic Land Vehicle in Neural Networl}, ALVINN, was the first self-driving car ever proposed, in 1989; it was based on neural networks responsible to detect lines, segment the environment, and drive the car.
While the general principles on which it was based worked well, the limited computed capabilities at the time, didn't make the solution take off. Furthermore, the absence of data meant that it was extremely difficult to gather the necessary samples and datasets on which to base the models that would manage vision and driving.


LiDAR stands for Light Detection And Ranging. It's a method to measure distance of object by firing a focused laser beam and measuring the time it takes for it to bounce back at the source after being reflected by something.
Compared to traditional camera images, LiDAR data can provide additional information about the surrounding scene...

A self-driving vehicle cannot rely on LiDAR alone, however. While this technology works great in most environments, including dark areas, if the scene surrounding the car becomes more noisy, due to rain or fog for example, the LiDAR sensor can become imprecise or fail.

For this reason, a third sensor is usually employed, the RADAR, which scans the surrounding area based on radio waves\dots

\section{2D Object detection}
IoU: intersection of union





\section{3D Object detection}
The goal of 3D object detection is to predict a three-dimensional bounding box around each object of interest.

Similarly to what happens with 2D detection, IoU can be used to evaluate the performance of a model.


\textbf{Formal definition}
\dots
In the case of self-driving vehicles specifically, we can assume a value of zero for roll and pitch since the car is "glued" to the road.
Frustum...



The most suited category of neural networks for image processing are Convolutional Neural Networks, CNNs\dots
CNNs can capture different patterns 

Region Based Convolutional Neural Networks, R-CNN, is a more scalable alternative to traditional CNNS. This approach, originally proposed by R. Girshick et al. \cite{DBLP:conf/cvpr/GirshickDDM14} in 2014\dots

CNNs have been successfully employed on point cloud data as well; PointRCNN


Different input modalities
\begin{itemize}
    \item Images only
    \item LiDAR data only
    \item Images + LiDAR data. With this approach, 
\end{itemize}


Only image: Monocular 3D Object Detection for Autonomous Driving and Geometru-based Distance Decompositon for Monocular 3D Object Detection or Pseudo-LiDAR

Only LiDAR: 

Both:





\newpage
\section{Adversarial attacks on neural networks}
Most vision-based recognition software on ADS is based on CNNS; often, CNN-based deep learning models are vulnerable to the so called adversarial, 

There can be small, pixel-level changes to an image that will cause the AI model to incorrectly interpret it or, on the other hand, a completely new image can be used to trick to model into thinking it is something else. The first kind is particularly dangerous, since such changes can be invisible to the human eye, and thus harder to detect.

There are mainly two categories of methods to achieve adversarial attacks, namely, optimization-based methods and fast gradient step method (FGSM)-based approach.


In \cite{DBLP:journals/iotj/ZhangLWWLJ22}, Zhang et al. propose and end-to-end evaluation framework for assessing the safety of a self driving deep learning model.



study on street signs: K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust Physical-World Attacks on
Deep Learning Visual Classification,” in 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition. IEEE, 2018, pp. 1625–1634.


In general, adversarial attacks are organized in three categories: evasion, poisoning, and extraction attacks:
\begin{itemize}
    \item Evasion attacks modify the input to a classifier such that it is misclassified, while keeping the modification as small as possible. Evasion attacks can be black-box or white-box: in the white-box case, the attacker has full access to the architecture and parameters of the classifier. For a black-box attack, clearly this is not the case.
    \item In poisoning attacks, attackers have the opportunity of manipulating the training data to significantly decrease the overall performance, cause targeted misclassification or bad behavior, and insert backdoors and neural trojans
    \item Extraction attacks aim to develop a new model, starting from a proprietary black-box model, that emulate the behavior of the original model.
\end{itemize}
