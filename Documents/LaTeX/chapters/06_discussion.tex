\chapter{Discussion}
\label{chap:6_discussion}
In this section we will discuss the obtained results and the implications of this study, as well as provide an answer to the RQs and analyze potential threats to its validity.

\section{Answers to Research Questions}
\subsection{Research Question 1}
In figures \ref{} and \ref{} we display the box plot charts for the distributions of the dependent variables with respect to \textit{Exp1} and its two tasks, and \textit{Exp2}.


\subsection{Research Question 2}

\subsection{Research Question 3}

\section{Implications}
In this section, we outline implications for lecturers and researchers based on the main findings from our exploratory investigation. 

\subsection{Implications for lecturers} 
The data, which seems to indicate that developers' productivity improves when using \tdd and that there is not a substantial difference between \tdd and \notdd in terms of external quality, suggest properly teaching \tdd. 
As for \textit{Exp2}, we also observed by means of the descriptive statistics that \tdd improves both external quality and developers' productivity. 
Results let us then speculate that \tdd and the used development pipeline \cite{TDDEC}- which suggests writing simple simulations of hardware components (mock objects) to let tests pass and to continue writing first tests and then production code for features of a given \es - can be taught in academic \es courses instead of a more traditional approach such as \notdd. 
To some extent, this would also have practical implications from the professional perspective. That is, if a newcomer to the working market is familiar with \tdd (and the studied development pipeline), the software industry could be encouraged to migrate their development from \notdd to \tdd. In fact, software companies could be encouraged to use \tdd because \textit{(i)} there is initial evidence that it improves productivity and \textit{(ii)} developers could be familiar with it before being hired and this reduces training and its related costs.


This suggests that future courses relating to \ess (even the ones not focusing on \tdd or \ess testing in general) should consider limiting the ... of mocked systems, in favor of more hardware deployment into their \dots. 

\subsection{Implications for researchers} 
The post-experiment data highlight that both the execution of the implementation tasks and the application of \tdd were perceived as more difficult by the participants. 
To some extent, this outcome is coherent with past studies \eg~{RomanoZBPS22,Romano:2020:XP}. 
A plausible explanation is that participants were experienced with unit testing in a test-last manner and less accustomed to writing tests before production code. 
This postulation suggests two possible future research directions: \textit{(i)} replicating our experiments  with more experienced developers to ascertain that the greater the experience with unit testing in a test-last manner, the more negative their perspective with \tdd is and \textit{(ii)} conducting a study with a cohort of developers to investigate how developers perception of \tdd applied to \es development changes over time.


\section{Threats to Validity}
According to Campbell and Stanley \cite{ResearchOfTeaching}, threats to validity are either internal or external. Internal validity concerns "controlling" the aspects of the experiment's setting to ensure that the outcomes are caused only by the introduced techniques \cite{DBLP:conf/icse/SiegmundSA15}. External validity refers to showing a real-world effect, but without knowing which factors actually caused the observed difference \cite{DBLP:conf/icse/SiegmundSA15}.
Cook and Campbell extended the list of validity categories to: conclusion, internal, construct, and external validity \cite{QuasiExp}. The latter classification has often been adopted in past empirical software engineering studies \cite{DBLP:books/sp/WohlinRHOR00}. 


In order to determine the potential threats to validity that may affect our studies, we referenced Wohlin \etal 's guidelines \cite{DBLP:books/sp/WohlinRHOR00}:
\begin{enumerate}
    \item \textbf{Internal validity} refers to the extent to which we can be confident that a cause-and-effect relationship established in a study cannot be explained by other factors; it makes the conclusions of a causal relationship credible and trustworthy. Without high internal validity, an experiment cannot demonstrate a causal link between two variables.
    There are three necessary conditions for internal validity and all three must occur to experimentally establish causality between an independent variable A (treatment variable) and dependent variable B (response variable): \textit{(i)}: the treatment and response variables change together; \textit{(ii)}: the treatment precedes changes in the response variables, and \textit{(iii)}: no confounding or extraneous factors can explain the results of the study.
    Threats to internal validity include: 
    \begin{itemize}
        \item \textbf{Deficiency of treatment setup}: the treatment setup is sometimes not appropriate, which may impact the results. For example, noise and tool performance could impact the results of a study, when they are not related to the treatment of the study.
        \item \textbf{Ignoring relevant factors}: factors not considered in the experiment setup sometimes impact the study results, such as the usability of the tools used in the experiment and their performance.
        \item \textbf{History}: A study composed of a set of treatments applied at different occasions may be impacted by the history threat. Treatments may be given to the same object at several occasions, each of which is associated with specific circumstances, such as time and location. The change in circumstances may impact the results.
        \item \textbf{Maturation:} the subjects may react differently as time passes while they perform the treatment: some may become bored and others may become motivated.
        \item \textbf{Testing}: the subjects may behave differently towards the treatment if they do it several times: they may learn the results and adapt their responses accordingly.
        \item \textbf{Treatment design}: the artifacts used in the treatment, such as the data collection form and the documents used as information source, could affect the results if not well-designed and tested.
        \item \textbf{Subject selection}: subjects for studies are selected to represent a population. The selection method affects the results and their interpretation. The group of subjects that participate in a study is always heterogeneous. The difference between individuals should not be the dominant factor for the study results: the treatment should be the dominant factor.
        \item \textbf{Sample selection}: data is usually collected from data sources that represent the context of the study, such as NVD database, or open-source logs and artifacts. The data sample should be representative of the studied type of data.
        \item \textbf{Incompleteness of data}: researchers often use heuristics or keyword searches to select records from data sources that represent the data required for the given study. These techniques may fail to identify all the expected records from the data sources.
        \item \textbf{Mortality}: some subjects selected for a given treatment may drop out of the treatment. This should be considered when evaluating the impact of the given treatment on the subjects. Drop-out subjects should be removed from the treatment.
        \item \textbf{Imitation of treatment}: this applies to studies that require different subjects/groups to apply different methods/techniques and use the responses to compare the methods and techniques. The subjects/groups may provide responses influenced by their experience and knowledge about the evaluated methods if they learn that these methods/techniques are being applied by other subjects/groups.
        \item \textbf{Motivation}: a subject may be motivated or resistant to use a new approach/method/technique. This may affect their response/performance in applying either the old or the new approach/method/technique
    \end{itemize}

    \item \textbf{External validity} represents the extent to which we can generalize the findings of a study to other measures, settings or groups. In other words, can we apply the findings of your study to a broader context?
    Threats to external validity include: 
    \begin{itemize}
        \item \textbf{Representation of the population}: the selected subjects/groups should represent the population that the study applies to.
        \item \textbf{Representation of the setting}: the setting of the study should be representative of the study goal. For example, tools used in the study should represent a real setting, not old ones.
        \item \textbf{Interaction of selection and treatment}: 
        \item \textbf{Context of the study}: The time and location of the study impacts the ability to generalize its results.
    \end{itemize}


    \item \textbf{Construct validity} refers to the extent to which a study's measures are related to the theoretical construct being studied. 
    Threats to construct validity include:
    \begin{itemize}
        \item \textbf{Theory definition}: the measured variables may not actually measure the conceptual variable. An experiment derived from an insufficiently defined theory does not represent the theory.
        \item \textbf{Mono-operation bias}: the study should include more than one independent variable, one treatment, and one subject. Discovering a phenomenon from one variable, case, or subject implies that a theory may exist but may not confirm the theory.
        \item \textbf{Mono.method bias}: using only one metric to measure a variable results in a measurement bias that can mislead the experiment.
        \item \textbf{Appropriateness of data}: researchers often use heuristics or keyword searches to select records from data sources. These techniques may result in the extraction of records that are not related to the given study.
        \item \textbf{Experimenter bias}: this happens when a researcher classifies artifacts /data based on his/her own perception or understanding rather than an objective metric. The perception may not be correct.
        \item \textbf{Measurement metrics}: the measurement method and the details of the measurement impact the study results.
        \item \textbf{Interaction with different treatments}: a subject that participates in a set of treatments may provide biased responses; his/her responses could be impacted by the interactions of the treatments of the study.
        \item \textbf{Treatment testing}: a study construction needs to be tested for quality assurance. However, the responses of subjects participating in the study test are affected by their experience with the treatment.
        \item \textbf{Hypothesis guessing}: some subjects try to figure out the intended outcomes of studies they are involved in and adapt their responses based on their guesses.
        \item \textbf{Evaluation apprehension}: subjects may behave differently when evaluated, \eg review their code more thoroughly. This impacts the truth of the evaluated responses.
        \item \textbf{Experimenter expectations}: the subjects may have expectations of the experiment and may provide answers accordingly. The study should formulate the treatment to mitigate that, such as asking the questions in different ways.
    \end{itemize}
    
    
    \item \textbf{Conclusion validity} refers to the extent to which the conclusions drawn from a study are supported by the data. 
    Threats to conclusion validity include:
    \begin{itemize}
        \item \textbf{Statistical validity}: statistical tests have confidence and power, which indicate the ability of the test to assert a true pattern. Low confidence (or power) implies that the results are not conclusive and don't permit deriving conclusions.
        \item \textbf{Statistical assumptions}: some statistical tests and methods (\eg prediction and forecasting) use assumptions, such as normality and independence of the data, or independence of the variables. Violations or absence of tests for the assumptions for a given test/method threaten the ability to use the given test/algorithm.
        \item \textbf{Lack of expert evaluation}: interpreting the results often requires having deep knowledge about the context of the collected data. The results may also include critical hidden facts, which only experts can point out.
        \item \textbf{Fishing for results}: fishing for specific results (often results that conform to the researcher hypotheses) impacts the study setup and design. The researcher could "unintentionally" draw conclusions that are not correct for the study setup and design.
        \item \textbf{Reliability of the measures}: measurements of independent variables should be reliable: measuring the concept twice should provide the same result. Questionnaire wording is an example of causes of this threat.
        \item \textbf{Reliability of treatment implementation}: The implementation of the treatment should follow a standard, and it should be the same for all subjects.
        \item \textbf{Random heterogeneity of participants}: the participants to the study do not have a uniform background on the topics the study is aimed at.
        \item \textbf{Lack of data preprocessing}: The quality of raw data is often not excellent. Researchers need to explore them to identify problems, such as missing data, outliers, and wrong data values, e.g., values that do not follow the codification rules.
    \end{itemize}
\end{enumerate}

Completely avoiding/mitigating threats is often unfeasible, given the dependency between some of them: avoiding/mitigating a kind of threat (\ie in internal validity) might intensify or even introduce another kind of threat \cite{DBLP:books/sp/WohlinRHOR00}. As a result, there are inherent trade-offs between validities: with internal and external validities for example, the more we control extraneous factors in our study, the less we can generalize our findings to a broader context. As for our baseline and replication studies, let's consider the different kinds of threat individually.

\subsection{Threats to internal validity}.
The main threat in this category is perhaps related to the monitoring of the participants during the replication study; since they accomplished the implementation of the final task at home and alone, before deploying it on hardware under our supervision, we cannot be sure of the means employed by the participants to accomplish the task. Given the fact that the final score of the \textit{Embedded Systems} course was not influenced in any way by the outcome of the task, we can reasonably assume that the participants would have no reason to maliciously interfere with the task (\ie by cheating or sharing information). On the other hand, and entering the domain of the \textit{maturation} and \textit{motivation} threats, towards the end of the study, some participants might have grown bored with the procedure or might have received a less desirable treatment, and thus decided to not perform the same way they would have if they were more engaged. The threat of \textit{motivation} holds in all three experimental tasks, since it is related to the nature of the adopted experimental design; as for the threat of \textit{maturation} we see it holding especially in the replication study, since participants were not in a controlled environment.

The opposite, however, might also be true: given the fact that a volunteer population is generally more motivated and engaged compared to the average population \cite{DBLP:books/sp/WohlinRHOR00}, a \textit{subject selection} threat might have affected the overall results of the experiments.

\subsection{Threats to external validity}.
The main external validity threat could be the one of \textit{interaction of selection and treatment}: since both Bachelor's and Master's student were involved in the study, some of the latter with no prior testing experience or even without a strong Computer Science background, the results could potentially not be applicable to professional developers. 
As for the threat of \textit{representation of the setting}, while one might be concerned that the tools and hardware platform used in the studies (\ie \textit{Python}, the \textit{PyCharm IDE}, and \textit{Raspberry Pi}) are not really representative of the technologies that make up the majority of \ess, they are quite popular implementation tools for these systems. 

\subsection{Threats to construct validity}.
Although we did not disclose the purpose of our study to the participants during the experimental tasks, they might have tried to guess it, and adapted their behavior accordingly, arising a threat of \textit{hypotheses guessing}. 
Besides this, the threat of \textit{evaluation apprehension} should have been fairly mitigated, since the participants knew that they would be awarded the bonus score for the course regardless of their performance in the study.
From a \textit{theory definition} thread standpoint, the used dependent variables have been employed in a number of empirical studies of this kind in the past, thus we can safely assume that they are consolidated by this point. Finally, since no participant had a practical experience with \tdd, a threat of \textit{treatment testing} does not concern our study.

\subsection{Threats to conclusion validity}
In order to mitigate any potential threat of \textit{random heterogeneity of participants}, before staring the experimental tasks, we trained the participants with a series of frontal lectures and exercise, in order to uniform their knowledge on the techniques and technologies that they would have later used and make the two groups as homogeneous as possible. 
A threat of \textit{reliability of treatment implementation} might have occurred in tasks 1 and 2. For example, some participants might have followed \tdd more strictly than others; however, this should equally affect both experimental groups in the end and can thus be ignored. 
