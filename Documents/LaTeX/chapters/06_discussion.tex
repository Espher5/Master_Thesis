\chapter{Discussion}
\label{chap:6_discussion}
In this last chapter before the conclusions we will provide an answer to the defined research questions and discuss the implications of this study; finally, we will analyze potential threats to its validity.

\section{Answers to the research questions}
After examining the data emerged from the dependent variables' evaluation, the meta-analysis, and the thematic analyses for both post-questionnaires and final interviews, we can provide an answer to the research questions defined in Chapter \ref{chap:5_approach}.



\subsection{Research question 1}
When we introduced our dependent variables, besides $QLTY$, we defined three additional variables to estimate the external quality of software: $CYC$, $COG$, and $LOC$; however, as we have seen in the previous chapter, the values for these variables did not display any significant difference between the two approaches. As a result, to answer our first research question, we only considered the $QLTY$ variable, which highlighted more interesting results.

For $QLTY$, the comparison between \tdd and \notdd seems in favor of the former for any of the experimental tasks. However, the effects of \tdd are not uniform across the tasks: by looking at \textit{Exp1} the values for $QLTY$ represented by the width of the boxes in the plot charts is very similar across the two tasks (see figures \ref{box_plots_task1}.a and \ref{box_plots_task2}.a); this is further highlighted by the mean and median values. 
On the other hand, by looking at the box plot for $QLTY$ in \textit{Exp2} (see figure \ref{box_plots_task3}), we notice how the difference in width between the boxes is wider in this case. This suggests a moderating role of the experimental task.
As for the meta-analysis (see figure \ref{fp_qlty_prod}.a), the joint SMD for $QLTY$ is still in favor of \tdd and small (0.480).
Finally, considering how there might be cases, such as \textit{CR}, where a more complex \es, as expressed by the participants, still made the \tdd group produce higher values for $QLTY$ with respect to the \notdd group, while struggling in other areas, such as productivity, further strengthens the idea that using \tdd improves the external quality of the implemented \es.




\subsection{Research question 2}
Considering $PROD$, the effects of \tdd are contrasting across the experimental tasks: while the situation is similar for the first and final tasks, where the results for the box plot charts relative to the \tdd group are either higher than or comparable to the \notdd results (see figures \ref{box_plots_task1}.b and \ref{box_plots_task3}.b), for the second task in \textit{Exp1} the situation is the opposite, suggesting a moderating role of the experimental task, and highlighting how a harder task impacts developers' productivity.
The overall SMD for $PROD$ in the meta-analysis is in favor of \tdd (0.120), but it is negligible, suggesting an insignificant impact of \tdd on developers' productivity.

Given the fact that participants expressed how \textit{CR} was much harder compared to the first task (\ie \textit{IO}), and considering that \textit{G2} applied \tdd for the first time during this period, the obtained result is less surprising.
By looking at the task for \textit{Exp2}, on the other hand we can notice how the gap between \tdd and \notdd (in favor of the former) grew compared to the first task. Keep in mind that by the point participants tackled \textit{SH}, they had all applied \tdd exactly once, in a previous task. 
Consequently, we can speculate how gaining more experience with \tdd might balance the results, or even show an edge for this approach.



\subsection{Research question 3}
As for the number of written tests, reported by the $TEST$ variable, by looking at the box plot charts for the first and final experimental tasks (\ie \textit{IO} and \textit{SH}), we can see how the boxes for the \tdd group are not only much higher, but much shorter than the ones for the \notdd group; this is due to the fact that all participants that applied \tdd wrote a similar number of test cases compared to the \notdd group (for which some participants did not write any tests).
This is further highlighted by the mean values, which are 9.5 for \tdd and 3.8 for \notdd in \textit{IO}, and 11.6 for \tdd and 4.5 for \notdd in \textit{SH}, respectively.
On the other hand, in task2 of \textit{Exp1}, \textit{CR}, it is shown how a harder task, as voiced by the participants, impacted once again the \tdd group, resulting in a lower number of written test cases on average, with a mean of 5.4 for \tdd versus a mean of 9 for \notdd. However, the results for \textit{CR} also produced a higher maximum value for the $TEST$ variable; this may have resulted from a participant being more familiar with \tdd, who applied this approach more extensively.
By aggregating both tasks in \textit{Exp1} we can however see how on average \tdd still produced a higher number of tests.
Additionally, the meta-analysis for this variable produced an overall SMD in favor of \tdd and medium (0.600).
This suggests that employing \tdd may result in a higher number of test cases written when developing \ess.





\section{Implications}
In this section, we outline implications for lecturers and researchers based on the main findings from our assessment. 

\subsection{Implications for lecturers}
The data, which seems to indicate that overall external quality and number of test cases written improve when using \tdd and that there is not a substantial difference between \tdd and \notdd in terms of productivity, suggests properly teaching \tdd in the context of \es development. Furthermore, as for the first task of \textit{Exp1} and the one in \textit{Exp2}, we also observed by means of the descriptive statistics that \tdd improves the productivity of the developers for their produced solution.

These results also allow us to speculate that \tdd and the used development pipeline \cite{TDDEC} - which suggests writing simple simulations of hardware components (\ie mock objects) to let tests pass and to continue writing first tests and then production code for features of a given \es - can be taught in academic \es courses instead of a more traditional approach such as \notdd.

To some extent, this would also have practical implications from the professional developers' perspective: that is, if a newcomer to the working market is familiar with \tdd (and the referenced development pipeline for \ess), the software industry could be encouraged to migrate their development from \notdd to \tdd. In fact, software companies could be encouraged to use \tdd because \textit{(i)} there is initial evidence that it improves productivity and number of written test cases, and \textit{(ii)} developers could be familiar with it before being hired, thus reducing the amount of training required to gain a higher level of proficiency with this technique, as well as its related costs.

Finally, participants in our study appreciated their overall experience in studying \tdd, especially the deployment and testing of the developed solutions in a real target environment of their \es. 
We can postulate that this positive experience has affected the participants' intentions to use \tdd in the development of future \ess: this is practically relevant for lecturers that should plan courses on the design and the implementation of \ess that make a mix of theory and practical experiences for the students.

\subsection{Implications for researchers} 
The post-experimental data we gathered highlights how both the execution of the implementation tasks and the application of \tdd were perceived as more difficult by the participants. 
To some extent, this outcome is coherent with past studies (\eg \cite{DBLP:conf/xpu/0001SBFC20, DBLP:conf/esem/RomanoZBPS22}), with a plausible explanation being that participants were experienced with unit testing in a test-last manner and therefore were less prone to learn a new approach which expected them to write test cases before production code. 

This postulation suggests two possible future research directions: \textit{(i)} replicating our experiments with the participation of more experienced developers to ascertain that the greater the experience with unit testing in a test-last manner, the more negative their perspective with \tdd is and \textit{(ii)} conducting a longitudinal study with a cohort of developers to investigate how developers perception and retainment of \tdd applied to \es development changes over time, as well as determining how this changes affect the constructs we analyze (\ie external quality, developers' productivity, and number of test cases).

Quantitative and qualitative results suggest that the development pipeline proposed by Greening\cite{TDDEC} is a valuable alternative to a traditional way of coding, where production code is written before tests; researchers could be interested in further studying this pipeline (\ie "\textit{The Embedded TDD Cycle}") by exploiting field studies. 
Our preliminary results have the merit to justify such kinds of studies; moreover, we make our raw data publicly available online in order to allow, in the future, researchers to conduct secondary studies (\eg meta-analyses) on \tdd applied to \ess development.
We also made our replication package, including the raw data, available online to allow researchers to replicate our study and ease the execution of secondary studies (\eg meta-analyses) on \tdd applied to \ess development \cite{RepPackage}.

\section{Threats to Validity}
According to Campbell and Stanley \cite{ResearchOfTeaching}, threats to validity are either internal or external: internal validity concerns "controlling" the aspects of the experiment's setting to ensure that the outcomes are caused only by the introduced techniques \cite{DBLP:conf/icse/SiegmundSA15}, while external validity refers to showing a real-world effect, but without knowing which factors actually caused the observed difference \cite{DBLP:conf/icse/SiegmundSA15}.
Cook and Campbell extended the list of validity categories to: conclusion, internal, construct, and external validity \cite{QuasiExp}. The latter classification has often been adopted in past empirical software engineering studies \cite{DBLP:books/sp/WohlinRHOR00}. 


In order to determine the potential threats to validity that may affect our studies, we referenced Wohlin \etal's guidelines \cite{DBLP:books/sp/WohlinRHOR00}:
\begin{enumerate}
    \item \textbf{Internal validity} refers to the extent to which we can be confident that a cause-and-effect relationship established in a study cannot be explained by other factors; it makes the conclusions of a causal relationship credible and trustworthy. Without high internal validity, an experiment cannot demonstrate a causal link between two variables.
    There are three necessary conditions for internal validity and all three must occur to experimentally establish causality between an independent variable A (treatment variable) and dependent variable B (response variable): \textit{(i)}: the treatment and response variables change together; \textit{(ii)}: the treatment precedes changes in the response variables, and \textit{(iii)}: no confounding or extraneous factors can explain the results of the study.
    Threats to internal validity include: 
    \begin{itemize}
        \item \textbf{Deficiency of treatment setup}: the treatment setup is sometimes not appropriate, which may impact the results.
        \item \textbf{Ignoring relevant factors}: factors not considered in the experiment setup sometimes impact the study results.
        \item \textbf{History}: a study composed of a set of treatments applied at different occasions may be impacted by the history threat. Treatments may be given to the same object at several occasions, each of which is associated with specific circumstances, such as time and location.
        \item \textbf{Maturation:} the subjects may react differently as time passes while they perform the treatment: some may become bored and others may become motivated.
        \item \textbf{Testing}: the subjects may behave differently towards the treatment if they do it several times: they may learn the results and adapt their responses accordingly.
        \item \textbf{Treatment design}: the artifacts used in the treatment could affect the results if not well-designed and tested.
        \item \textbf{Subject selection}: subjects for studies are selected to represent a population which is not heterogeneous. The selection method affects the results and their interpretation.
        \item \textbf{Sample selection}: data is usually collected from data sources that represent the context of the study. The data sample should be representative of the studied type of data.
        \item \textbf{Incompleteness of data}: researchers often use heuristics or keyword searches to select records from data sources that represent the data required for the given study. These techniques may fail to identify all the expected records from the data sources.
        \item \textbf{Mortality}: some subjects selected for a given treatment may drop out of the treatment. These subjects should be removed from the treatment.
        \item \textbf{Imitation of treatment}: this applies to studies that require different subjects/groups to apply different methods/techniques and use the responses to compare the methods and techniques. The subjects/groups may provide responses influenced by their experience and knowledge about the evaluated methods if they learn that these methods/techniques are being applied by other subjects/groups.
        \item \textbf{Motivation}: a subject may be motivated or resistant to use a new approach/technique. This may affect their response/performance in applying either the old or the new approach/technique.
    \end{itemize}

    \item \textbf{External validity} represents the extent to which we can generalize the findings of a study to other measures, settings or groups. In other words, can we apply the findings of your study to a broader context?
    Threats to external validity include: 
    \begin{itemize}
        \item \textbf{Representation of the population}: the selected subjects/groups should represent the population that the study applies to.
        \item \textbf{Representation of the setting}: the setting of the study should be representative of the study goal.
        \item \textbf{Interaction of selection and treatment}: results produced by the participants of a study may not generalize outside the study population.
        \item \textbf{Context of the study}: The time and location of the study impacts the ability to generalize its results.
    \end{itemize}


    \item \textbf{Construct validity} refers to the extent to which a study's measures are related to the theoretical construct being studied. 
    Threats to construct validity include:
    \begin{itemize}
        \item \textbf{Theory definition}: the measured variables may not actually measure the conceptual variable. An experiment derived from an insufficiently defined theory does not represent the theory.
        \item \textbf{Mono-operation bias}: the study should include more than one independent variable, one treatment, and one subject. Discovering a phenomenon from one variable, case, or subject implies that a theory may exist but may not confirm the theory.
        \item \textbf{Mono-method bias}: using only one metric to measure a variable results in a measurement bias that can mislead the experiment.
        \item \textbf{Appropriateness of data}: researchers often use heuristics or keyword searches to select records from data sources. These techniques may result in the extraction of records that are not related to the given study.
        \item \textbf{Experimenter bias}: this happens when a researcher classifies artifacts /data based on his/her own perception or understanding rather than an objective metric. The perception may not be correct.
        \item \textbf{Measurement metrics}: the measurement method and the details of the measurement impact the study results.
        \item \textbf{Interaction with different treatments}: a subject that participates in a set of treatments may provide biased responses; his/her responses could be impacted by the interactions of the treatments of the study.
        \item \textbf{Treatment testing}: a study construction needs to be tested for quality assurance. However, the responses of subjects participating in the study test are affected by their experience with the treatment.
        \item \textbf{Hypothesis guessing}: some subjects try to figure out the intended outcomes of studies they are involved in and adapt their responses based on their guesses.
        \item \textbf{Evaluation apprehension}: subjects may behave differently when evaluated, \eg review their code more thoroughly. This impacts the truth of the evaluated responses.
        \item \textbf{Experimenter expectations}: the subjects may have expectations of the experiment and may provide answers accordingly. The study should formulate the treatment to mitigate that, such as asking the questions in different ways.
    \end{itemize}
    
    
    \item \textbf{Conclusion validity} refers to the extent to which the conclusions drawn from a study are supported by the data. 
    Threats to conclusion validity include:
    \begin{itemize}
        \item \textbf{Statistical validity}: statistical tests have confidence and power, which indicate the ability of the test to assert a true pattern. Low confidence (or power) implies that the results are not conclusive and don't permit deriving conclusions.
        \item \textbf{Statistical assumptions}: some statistical tests and methods use assumptions, such as normality and independence of the data, or independence of the variables. Violations or absence of tests for the assumptions for a given test/method threaten the ability to use the given test/algorithm.
        \item \textbf{Lack of expert evaluation}: interpreting the results often requires having deep knowledge about the context of the collected data. The results may also include critical hidden facts, which only experts can point out.
        \item \textbf{Fishing for results}: fishing for specific results (often results that conform to the researcher hypotheses) impacts the study setup and design. 
        \item \textbf{Reliability of the measures}: measurements of independent variables should be reliable: measuring the concept twice should provide the same result. Questionnaire wording is an example of causes of this threat.
        \item \textbf{Reliability of treatment implementation}: The implementation of the treatment should follow a standard, and it should be the same for all subjects.
        \item \textbf{Random heterogeneity of participants}: the participants to the study do not have a uniform background on the topics the study is aimed at.
        \item \textbf{Lack of data preprocessing}: The quality of raw data is often not excellent. Researchers need to explore them to identify problems, such as missing data, outliers, and wrong data values, e.g., values that do not follow the codification rules.
    \end{itemize}
\end{enumerate}

Completely avoiding/mitigating threats is often unfeasible, given the dependency between some of them: avoiding/mitigating a kind of threat (\ie in internal validity) might intensify or even introduce another kind of threat \cite{DBLP:books/sp/WohlinRHOR00}. As a result, there are inherent trade-offs between validities: with internal and external validities for example, the more we control extraneous factors in our study, the less we can generalize our findings to a broader context. As for our baseline and replication experiments, in the next subsections we will consider the different kinds of threat individually.

\subsection{Threats to internal validity}
The main threat in this category is perhaps related to the monitoring of the participants during the replication study; since they accomplished the implementation of the final task at home and alone, before deploying it on hardware under our supervision, we cannot be sure of the means employed by the participants to accomplish the task. Given the fact that the final score of the \textit{Embedded Systems} course was not influenced in any way by the outcome of the task, we can reasonably assume that the participants would have no reason to maliciously interfere with the task (\ie by cheating or sharing information). On the other hand, and entering the domain of the \textit{maturation} and \textit{motivation} threats, towards the end of the study, some participants might have grown bored with the procedure or might have received a less desirable treatment, and thus decided to not perform the same way they would have if they were more engaged. The threat of \textit{motivation} holds in all three experimental tasks, since it is related to the nature of the adopted experimental design; as for the threat of \textit{maturation} we see it holding especially in the replication study, since participants were not in a controlled environment.

The opposite, however, might also be true: given the fact that a volunteer population is generally more motivated and engaged compared to the average population \cite{DBLP:books/sp/WohlinRHOR00}, a \textit{subject selection} threat might have affected the overall results of the experiments.

\subsection{Threats to external validity}
The main external validity threat could be the one of \textit{interaction of selection and treatment}: since only students were involved in the study, some with no prior testing experience or even without a strong Computer Science background, the results could potentially not be applicable to professional developers. 
However, the use of students has the advantage that they have more homogeneous backgrounds and skills and allow obtaining initial empirical evidence \cite{DBLP:conf/metrics/CarverJMS03, DBLP:journals/ese/HostRW00}.
As for the threat of \textit{representation of the setting}, while one might be concerned that the implementation tasks and tools used in the studies are not really representative of the technologies that make up the majority of \ess, they are quite popular implementation tools for these systems. 
However, we opted for \ess (\ie \textit{IO}, \textit{CR}, and \textit{SH}) that can be considered representative of the domain of interest for our study. Furthermore, to deal with external validity, we also asked the participants to base the \ess implementation on a recognized development pipeline \cite{TDDEC}. Finally, we asked the participants in \textit{Exp2} to deploy and test the \es (\ie \textit{SR}) in a real hardware environment.

\subsection{Threats to construct validity}
We measured each construct with a single dependent variable (\eg external quality with $QLTY$). As so, in case of measurement bias,
this might affect the obtained results with a threat of \textit{mono-method bias}. 
Although we did not disclose the purpose of our study to the participants during the experimental tasks, they might have tried to guess it, and adapted their behavior accordingly, arising a threat of \textit{hypotheses guessing}. 
Besides this, the threat of \textit{evaluation apprehension} should have been fairly mitigated, since the participants knew that they would be awarded the bonus score for the course regardless of their performance in the study.
From a \textit{theory definition} thread standpoint, the used dependent variables have been employed in a number of empirical studies of this kind in the past, thus we can safely assume that they are consolidated by this point. Finally, since no participant had a practical experience with \tdd, a threat of \textit{treatment testing} does not concern our study.

\subsection{Threats to conclusion validity}
In order to mitigate any potential threat of \textit{random heterogeneity of participants}, before staring the experimental tasks, we trained the participants with a series of frontal lectures and exercise, in order to uniform their knowledge on the techniques and technologies that they would have later used and make the two groups as homogeneous as possible. 
A threat of \textit{reliability of treatment implementation} might have occurred in tasks 1 and 2. For example, some participants might have followed \tdd more strictly than others; however, this should equally affect both experimental groups in the end and can thus be ignored. 
